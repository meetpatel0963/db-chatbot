page_id,summary,passages
557164,"Chat Module It calls the EvaluateInput method by which End user can interact with our pretrained model. EvaluateInput Method It keeps taking input from the user until specific ending criteria has been met. It will normalize the user input(Preprocessing on input sentence) and will call the evaluate method in order to get predicted output which will be shown to the user as the bot's response. Evaluate Method This method will convert input sentence to numeric tensor, and it will pass this tensor, INPUT_LENGTH and MAX_LENGTH to GreedySearchDecoder module to predict the output. It will then fetch actual words from obtained IDs of words. It will return the final output sentence.  |    |  ","Chat Module It calls the EvaluateInput method by which End user can interact with our pretrained model.###SEP###EvaluateInput Method It keeps taking input from the user until specific ending criteria has been met. It will normalize the user input(Preprocessing on input sentence) and will call the evaluate method in order to get predicted output which will be shown to the user as the bot's response.###SEP###Evaluate Method This method will convert input sentence to numeric tensor, and it will pass this tensor, INPUT_LENGTH and MAX_LENGTH to GreedySearchDecoder module to predict the output. It will then fetch actual words from obtained IDs of words. It will return the final output sentence.###SEP######SEP###"
557133,"Decoder is a stack of several recurrent units each of which predicts an output at time step t. Hidden states from the last timestep of the encoder are given as an input to the decoder. OUTPUT_SIZE = To map each word in the input sequence to a d-dimensional vector, where d = hidden size. DROPOUT = To specify dropout ratio in RNN units. Now, a final embedded sequence is passed through RNN (a stack or several LSTM or GRU) along with hidden states from the previous time step. Training During training, we have the actual output or a target sequence. This technique is called “Teacher forcing”. We are forcing the Decoder to learn from the actual target rather than its predicted value. Testing (Inference) In the real world, when a user asks for a query we don’t have the actual output. The predicted output (word) is fed to the next time step as an input. Rest is all the same as in the training phase. Teacher Forcing In Teacher forcing, we Force our Model to learn.  |    |  ","Decoder is a stack of several recurrent units each of which predicts an output at time step t. Hidden states from the last timestep of the encoder are given as an input to the decoder. We have implemented a Decoder class. First step is to initialize the hidden size, output size, embedding layer, dropout, RNN and Linear layer. HIDDEN_SIZE = It is the size of the embedding vector to which each word will be mapped. OUTPUT_SIZE = To map each word in the input sequence to a d-dimensional vector, where d = hidden size. EMBDEDDING_SIZE = To map each word in the input sequence (input to decoder - target in question-answer pairs) to a d-dimensional vector, where d = hidden size. DROPOUT = To specify dropout ratio in RNN units. RNN = To create an LSTM or GRU layer based on the RNN_cell_type argument. Its value will be either ‘LSTM’ or ‘GRU’. LINEAR_LAYER = To specify a fully-connected layer to map our outputs to a vector of size = number of words in the vocabulary.###SEP###Process First, we pass our input sequence of the decoder (initially, just the <SOS> token) through an embedding layer and obtain a d-dimensional vector, where d = hidden size. This embedded vector is passed through an embedding dropout layer. It will apply word-level dropout, meaning that all occurrences of some of the words will be dropped during forward and backward pass in order to reduce overfitting. Now, a final embedded sequence is passed through RNN (a stack or several LSTM or GRU) along with hidden states from the previous time step. The output from the last timestep of the decoder is passed through a fully-connected layer which will give us the output vector of size = number of words in the vocabulary. This output vector contains the relevance score of each word in the vocabulary. It is passed through the softmax layer to get the probability values for each word to occur at the current position in the output. Decoder works differently during training and inference, unlike the Encoder.###SEP###Training During training, we have the actual output or a target sequence. Thus, we can feed the true output token from the previous time step to the current time step. This technique is called “Teacher forcing”. We are forcing the Decoder to learn from the actual target rather than its predicted value. Teacher forcing works significantly better than using the model output as an input to the next time step.###SEP###Testing (Inference) In the real world, when a user asks for a query we don’t have the actual output. Thus, we cannot use Teacher forcing strategy during inference. The predicted output (word) is fed to the next time step as an input. Rest is all the same as in the training phase.###SEP###Teacher Forcing In Teacher forcing, we Force our Model to learn. While training our model, in Decoder RNN each unit requires a token/word predicted by the previous unit as an input. At the early stages of training, the predictions of the model will be very bad. If we don’t use Teacher Forcing, the hidden states of the model will be updated by a sequence of wrong predictions, errors will accumulate, and it will be difficult for the model to learn from that. So while training the model, We will feed the target word with some probability and by doing so our model will learn much faster.###SEP######SEP###"
557106,"In the twitter dataset the data is stored as question with response id, as we would be needing the data in form of pair of questions and answers, hence we need to preprocess the data also we need to clean the data as well because it contains urls, hashtags, emojis, non-english sentences and much more. Approach To clean the data we have implemented DataCleaner class. It contains following methods: Replace_shortforms: It will replace the short forms like I’m with I am in the data. Remove_url_emojies_names: It removes url,emojis and names from the data. Remove_non_english_words: It removes the non-english words. Data_cleaner: This method takes questions and answers and then uses all_cleaners method to clean the text. To convert the data into our required format we have implemented DataSeperator class. It contains following methods: Clean_data: Uses the functionality provided by the DataCleaner class to clean the data. Format_data: This method joins the dataframe to convert the data into our desired format Separate_company_data: This method separates the data company wise and saves the csv using save_csv method of the class Save_csv: This method provides the functionality to save the csv of a company.  |    |  ","In the twitter dataset the data is stored as question with response id, as we would be needing the data in form of pair of questions and answers, hence we need to preprocess the data also we need to clean the data as well because it contains urls, hashtags, emojis, non-english sentences and much more.###SEP###Approach To clean the data we have implemented DataCleaner class. It contains following methods: Replace_shortforms: It will replace the short forms like I’m with I am in the data. Remove_url_emojies_names: It removes url,emojis and names from the data. Remove_non_english_words: It removes the non-english words. All_cleaners: This methods calls all the above method to clean the text. Data_cleaner: This method takes questions and answers and then uses all_cleaners method to clean the text. To convert the data into our required format we have implemented DataSeperator class. It contains following methods: Clean_data: Uses the functionality provided by the DataCleaner class to clean the data. Format_data: This method joins the dataframe to convert the data into our desired format Separate_company_data: This method separates the data company wise and saves the csv using save_csv method of the class Save_csv: This method provides the functionality to save the csv of a company.###SEP######SEP###"
491604,"Neural Networks do not understand language data directly. Therefore, we need to convert our text data into numerical vectors. Approach We have implemented our own Vocabulary Class. index2word - To convert numerical vectors into original text data. Along with this, it also keeps track of the number of unique words and counts of each word in the corpus and a flag “trimmed” to indicate whether the trim operation is applied or not. There are three functions: addWord: It takes a word as an argument and checks whether it is already present in the dictionary. Otherwise, add it to the dictionary and set reverse mapping. addSentence: It takes a sentence as an argument, splits it into words (i.e. delimiter = ‘ ‘) and applies addWord on each word of the sentence. trim: It takes the minimum count (THRESHOLD) of the word to be kept in the dictionary. SOS_token: To indicate the start of a sequence. Additionally, we have a utility function called “trimRareWords”. If any word in either input or output sentence in a particular pair has count less than minimum THRESHOLD, then that pair is discarded.  |    |  ","Neural Networks do not understand language data directly. They can only understand numerical data. Therefore, we need to convert our text data into numerical vectors.###SEP###Approach We have implemented our own Vocabulary Class. Basically, it contains two dictionaries: word2index - To map each word in the corpus to a unique index. index2word - To convert numerical vectors into original text data. Along with this, it also keeps track of the number of unique words and counts of each word in the corpus and a flag “trimmed” to indicate whether the trim operation is applied or not. There are three functions: addWord: It takes a word as an argument and checks whether it is already present in the dictionary. If it is, then increase its count by 1. Otherwise, add it to the dictionary and set reverse mapping. addSentence: It takes a sentence as an argument, splits it into words (i.e. delimiter = ‘ ‘) and applies addWord on each word of the sentence. trim: It takes the minimum count (THRESHOLD) of the word to be kept in the dictionary. All the words with count less than THRESHOLD are discarded. Initially, we have 3 tokens in our dictionary: PAD_token: To pad short sequences up to predefined maximum length. SOS_token: To indicate the start of a sequence. EOS_token: To indicate the end of a sequence. Additionally, we have a utility function called “trimRareWords”. It takes vocabulary objects, pairs of input and output sentences and minimum count of the word to be kept in the dictionary as arguments. If any word in either input or output sentence in a particular pair has count less than minimum THRESHOLD, then that pair is discarded. At the end, we create a new vocabulary with remaining pairs.###SEP######SEP###"
491540,"Seq2Seq Architecture At a high level, our task is to generate a sequence of words given an input sequence. In general, sequence-to-sequence models are appropriate where the length of the input and output sequences are not necessarily the same and the entire input sequence is required in order to predict the output sequence. Sequence-to-sequence models revolutionized the world of natural language processing. Its applications include Neural Machine Translation, Image Captioning, Conversational models, Text Summarization etc. The idea is to use two RNNs (Recurrent Neural Networks), one of which works as an encoder and the other as a decoder. Encoder Decoder Architecture The sequence-to-sequence model consists of 3 parts: Encoder, Context vector (encoder outputs) and Decoder.  |    |  ","Seq2Seq Architecture At a high level, our task is to generate a sequence of words given an input sequence. In general, sequence-to-sequence models are appropriate where the length of the input and output sequences are not necessarily the same and the entire input sequence is required in order to predict the output sequence. Sequence-to-sequence models revolutionized the world of natural language processing. Its applications include Neural Machine Translation, Image Captioning, Conversational models, Text Summarization etc. The idea is to use two RNNs (Recurrent Neural Networks), one of which works as an encoder and the other as a decoder.###SEP###Encoder Decoder Architecture The sequence-to-sequence model consists of 3 parts: Encoder, Context vector (encoder outputs) and Decoder.###SEP######SEP###"
459055,"Model Module It defines Seq2Seq method which creates the object of Encoder and Decoder class. If LoadFileName is provided then appropriate parameters will be loaded to encoder and decoder objects. It also defines an Embedding layer which basically maps the very high dimensional data to low dimensional data. TrainIters method will be called to start the training. TrainStep method resets the gradients for encoder and decoder, passes the batch through encoder, initializes decoder’s initial hidden state using encoder’s final hidden state. Finally, based on the randomized teacher forcing ratio, decoder will generate the output sequence for the given input sequence. At each timestamp of decoder, a model will compute NLLLoss (categorical cross- entropy) and accumulate this losses. Finally, this function will return an average loss and loss cache. In our task, we get the vector of size = (batch size X number of words in the vocab) from the decoder as an output/ This vector contains the probability values for each word and for each batch. In neural networks, our objective is to minimize the loss function in order to get to the global (or local) minima. Equivalent to maximizing the probability of choosing the correct word log(prob), we can minimize the negative log likelihood -log(prob). This will give us the loss values for each batch resulting in a vector of size = (batch size).  |    |  ","Model Module It defines Seq2Seq method which creates the object of Encoder and Decoder class. If LoadFileName is provided then appropriate parameters will be loaded to encoder and decoder objects. It also defines an Embedding layer which basically maps the very high dimensional data to low dimensional data. It returns an encoder, decoder and embedding layer.###SEP###Train Model It defines hyper parameters which will be used while training the model. TrainIters method will be called to start the training. In TrainIters method, for each batch of each epoch (iteration), TrainStep Method will be called. TrainStep method resets the gradients for encoder and decoder, passes the batch through encoder, initializes decoder’s initial hidden state using encoder’s final hidden state. Finally, based on the randomized teacher forcing ratio, decoder will generate the output sequence for the given input sequence. At each timestamp of decoder, a model will compute NLLLoss (categorical cross- entropy) and accumulate this losses. At the end, our model will apply backpropogation using backward() method and update the weights of the neural network using optimizer.step() method. Finally, this function will return an average loss and loss cache.###SEP###Loss Function In practice, the softmax function is used in tandem with the negative log- likelihood (NLL). Generally, this loss function is used in classification tasks. In our task, we get the vector of size = (batch size X number of words in the vocab) from the decoder as an output/ This vector contains the probability values for each word and for each batch. Now, we want to maximize the probability of choosing the correct word. In neural networks, our objective is to minimize the loss function in order to get to the global (or local) minima. Negative Log-likelihood function is given by: Loss(y) = -log(y) where, y is the probability for correct output. Equivalent to maximizing the probability of choosing the correct word log(prob), we can minimize the negative log likelihood -log(prob). We have implemented a function called “NLLLoss” to get the average loss for the current batch. It takes the output vector of the decoder containing the probabilities and target word at the current position for each batch. Now, we replace the vector of size = number of words for each batch by a single value containing a probability of target word and take the -log(prob) of that vector. This will give us the loss values for each batch resulting in a vector of size = (batch size). We take the mean loss of the entire batch and return it from the function. For each batch, NLLLoss will be called MAX_LENGTH times, where MAX_LENGTH = predefined maximum length of a sequence.###SEP######SEP###"
459048,"Embedding Layer When we convert the words of a sequence into one-hot-encoded form, it contains a lot of zeros. For example, if our vocabulary contains 50,000 words and an input sequence is “How are you?”, Only one entry of this vector is 1 and the remaining are 0’s. These embedding vectors capture some of the semantics of the input by placing similar inputs close together in the embedding space. These Embedding layers are used to reduce the dimension of the input sequence. LSTM and GRU Recurrent Neural networks suffer from short-term memory. For long enough sequences, RNNs can not propagate information from earlier time steps to later ones. Gradients are used to update a neural network’s weights. Due to the long dependencies, small gradient values will be multiplied together many times causing it to become extremely small. Thus, when we use these small gradients to update the network weights, the neural network stops learning. This can cause our network to forget important information, thus having a short-term memory. GRU has three gates: Reset gate, Update gate and Cell state. By using these gates, they can pass relevant information down the long chain of sequences.  |    |  ","Embedding Layer When we convert the words of a sequence into one-hot-encoded form, it contains a lot of zeros. For example, if our vocabulary contains 50,000 words and an input sequence is “How are you?”, then each word of the sequence will be converted to a vector of size 50,000. Only one entry of this vector is 1 and the remaining are 0’s. To tackle the problem of such sparse vectors leading to a lot of wasteful computations, we convert it to a smaller dimension vector called an “embedding vector”. These embedding vectors capture some of the semantics of the input by placing similar inputs close together in the embedding space. These Embedding layers are used to reduce the dimension of the input sequence.###SEP###LSTM and GRU Recurrent Neural networks suffer from short-term memory. For long enough sequences, RNNs can not propagate information from earlier time steps to later ones. During backpropagation, RNNs suffer from vanishing gradient problem. Gradients are used to update a neural network’s weights. Due to the long dependencies, small gradient values will be multiplied together many times causing it to become extremely small. Thus, when we use these small gradients to update the network weights, the neural network stops learning. This can cause our network to forget important information, thus having a short-term memory. To solve this problem, LSTM and GRU use gated architecture to keep track of the information to be kept or to be thrown away. LSTM has four gates: Forget gate, Input gate, Output gate and Cell state. GRU has three gates: Reset gate, Update gate and Cell state. By using these gates, they can pass relevant information down the long chain of sequences.###SEP######SEP###"
426076,"It is used to predict the output while testing the model. Basically the user's input will be passed through encoder RNN which will generate a context vector, then Decoder RNN will start predicting words one by one and at each time step it will greedily choose the word with highest probability. Firstly it will be initialized by basic encoder and decoder model’s objects. While testing/interacting with the user, we will convert the user's input into tensor(INPUT_SEQ) which then will be passed to this module along with MAX_LENGTH and INPUT_LENGTH in order to predict the output sequence. We will maintain two lists ALL_TOKENS and ALL_SCORES as well. DECODER_OUTPUT = Output of Decoder RNN DECODER_HIDDEN = Hidden state of Decoder RNN From DECODER_OUTPUT we can greedily fetch the token ID with highest probability.  |    |  ","It is used to predict the output while testing the model. Basically the user's input will be passed through encoder RNN which will generate a context vector, then Decoder RNN will start predicting words one by one and at each time step it will greedily choose the word with highest probability. Firstly it will be initialized by basic encoder and decoder model’s objects. While testing/interacting with the user, we will convert the user's input into tensor(INPUT_SEQ) which then will be passed to this module along with MAX_LENGTH and INPUT_LENGTH in order to predict the output sequence. INPUT_SEQ = Numeric tensor representing the input sentence MAX_LENGTH = No of words by which output sentence is bounded INPUT_LENGTH = Tensor representing the length of all input sentences in batch (Here only 1 length will be there as batch size will be 1) Now, in forward pass firstly we will call encoder RNN with INPUT_SEQ and INPUT_LENGTH as arguments, which will return outputs(ENCODER_OUTPUTS) and final hidden layer(ENCODER_HIDDEN) of encoder RNN. ENCODER_OUTPUTS = Outputs from all units of encoder RNN (which is of no use as of now) ENCODER_HIDDEN = final hidden state of encoder RNN, also known as context vector which contains information about input sentence When we start predicting the output, ENCODER_HIDDEN will act as the previous unit’s hidden state and as we wont have any predicted word, SOS_TOKEN will act as the previous predicted word. We will maintain two lists ALL_TOKENS and ALL_SCORES as well. ALL_TOKENS = all Greedily predicted words for given input sentence ALL_SCORES = Probabilities of predicted words/tokens Now all required variables have been initialized, and the model can start predicting words. Now for each step until MAX_LENGTH has been reached, we will forward pass through decoder RNN giving DECODER_INPUT, DECODER_HIDDEN and ENCODER_OUTPUTS as an argument, which will return DECODER_OUTPUT and DECODER_HIDDEN. DECODER_OUTPUT = Output of Decoder RNN DECODER_HIDDEN = Hidden state of Decoder RNN From DECODER_OUTPUT we can greedily fetch the token ID with highest probability. Predicted token ID and its score will be appended to ALL_TOKENS and ALL_SCORES respectively. This newly predicted token ID can be set to the decoder's input(DECODER_INPUT) and the old hidden state can be replaced with DECODER_HIDDEN for next time step. At the end, ALL_TOKENS and ALL_SCORES will be returned.###SEP######SEP###"
426066,"Encoder It is a stack of several recurrent units where each accepts a single token from input sequence and propagates it forward. EMBEDDING_LAYER = To map each word in the input sequence to a d-dimensional vector, where d = hidden size. Its value will be either ‘LSTM’ or ‘GRU’. The working of Encoder is the same in training and inference. Process First, we pass our input sequence through the embedding layer and obtain a d-dimensional vector, where d = hidden size. Now, these embedded vectors are passed through pack_paddded_sequence. For example, let the batch size is 6 and maximum sequence length is 9 as shown in the figure below: With this, we will have a matrix of size (6 x 9). Most of the computed results are thrown away since they are 0’s. We are using bi-directional LSTM (or GRU) cells in RNN. It takes two arguments: current sequences and hidden states from previous timestep. Thus, we need to add the outputs of both RNN units (one from left-to-right and the other from right-to-left). At the end, the final output and hidden states will be returned from the “forward” function of the Encoder class.  |  ###code### With this, we will have a matrix of size (6 x 9). Assuming that we have a weight matrix W of size (9 x 3), total number of computations will be: 6 * 9 = 54 multiplications and 6 * 8 = 48 additions. Most of the computed results are thrown away since they are 0’s. The actual required number of computations are as follows: :  9-mult  8-add ,  8-mult  7-add ,  6-mult  5-add ,  4-mult  3-add ,  3-mult  2-add ,  2-mult  1-add, ---------------, 32-mult  26-add, ------------------------------  , #savings: 22-mult & 22-add ops  ,           (54-32)  (48-26)   |  ","Encoder It is a stack of several recurrent units where each accepts a single token from input sequence and propagates it forward. Generally, LSTM (Long-Short Term Memory) or GRU (Gated Recurrent Unit) is used as a recurrent unit for better performance. We have implemented an Encoder class. First step is to initialize the hidden size, embedding layer, number of layers, dropout and RNN. HIDDEN_SIZE = It is the size of the embedding vector to which each word will be mapped. EMBEDDING_LAYER = To map each word in the input sequence to a d-dimensional vector, where d = hidden size. NUMBER_OF_LAYERS = To specify the number of recurrent units in RNN. DROPOUT = To specify dropout ratio in RNN units. For example, a dropout ratio of 0.2 will drop 20% of the neurons in the RNN randomly to reduce overfitting. RNN = To create an LSTM or GRU layer based on the RNN_cell_type argument. Its value will be either ‘LSTM’ or ‘GRU’. The working of Encoder is the same in training and inference. It accepts each token from the input sequence and passes the final hidden states to the decoder.###SEP###Process First, we pass our input sequence through the embedding layer and obtain a d-dimensional vector, where d = hidden size. Now, these embedded vectors are passed through pack_paddded_sequence. It takes two arguments: Embedded sequences and Input lengths. For example, let the batch size is 6 and maximum sequence length is 9 as shown in the figure below: With this, we will have a matrix of size (6 x 9). Assuming that we have a weight matrix W of size (9 x 3), total number of computations will be: 6 9 = 54 multiplications and 6 8 = 48 additions. Most of the computed results are thrown away since they are 0’s. The actual required number of computations are as follows: Thus, we can save a lot of computations even for this simple example. The functionality of pack_padded_sequence is shown in the figure below: As a result, we will get a tuple of tensors: the flattened sequences and the corresponding batch sizes ([6, 6, 5, 4, 3, 3, 2, 2, 1] for the above example). These packed sequences are passed through RNN that we defined earlier. We are using bi-directional LSTM (or GRU) cells in RNN. It takes two arguments: current sequences and hidden states from previous timestep. Now, outputs of RNN are passed through pad_packed_sequence. It is just the reverse process of pack_padded_sequence to obtain the original sized vectors from packed sequences. Since we are using bi-directional LSTM (or GRU), output size will be 2 hidden size. Thus, we need to add the outputs of both RNN units (one from left-to-right and the other from right-to-left). At the end, the final output and hidden states will be returned from the “forward” function of the Encoder class. The final hidden state produced from the encoder aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions. It acts as the initial hidden state of the decoder model.###SEP######SEP######code### With this, we will have a matrix of size (6 x 9). Assuming that we have a weight matrix W of size (9 x 3), total number of computations will be: 6 * 9 = 54 multiplications and 6 * 8 = 48 additions. Most of the computed results are thrown away since they are 0’s. The actual required number of computations are as follows: :  9-mult  8-add ,  8-mult  7-add ,  6-mult  5-add ,  4-mult  3-add ,  3-mult  2-add ,  2-mult  1-add, ---------------, 32-mult  26-add, ------------------------------  , #savings: 22-mult & 22-add ops  ,           (54-32)  (48-26) "
196755,"Go ahead, edit and customize this home page any way you like. We've added some sample content to get you started. Goal _Your space homepage should summarize what the space is for, and provide links to key resources for your team._ Core team Roadmap You can edit this roadmap or create a new one by adding the Roadmap Planner macro from the Insert menu. Link your Confluence pages to each bar to add visibility, and find more tips by reading the Atlassian blog: [Plan better in 2015 with the Roadmap Planner macro](http://blogs.atlassian.com/2015/01/roadmap-planner-macro/). A small team should plan to have a space for the team, and a space for each big project. The key is to think of a space as the container that holds all the important stuff - like pages, files, and blog posts - a team, group, or project needs to work.  |    |  0: Harvey Honner-whiteTeam Lead, 1: Alana Baczewski Tech Lead, 2: Sameer Farrell Marketing, 3: Mia Bednarczyk Recruitment  |   Link: Confluence 101: organize your work in spaces, Description: Chances are, the information you need to do your job lives in multiple places. Word docs, Evernote files, email, PDFs, even Post-it notes. It's scattered among different systems. And to make matters worse, the stuff your teammates need is equally siloed. If information had feelings, it would be lonely.But with Confluence, you can bring all that information into one place.  |  Link: Confluence 101: discuss work with your team, Description: Getting a project outlined and adding the right content are just the first steps. Now it's time for your team to weigh in. Confluence makes it easy to discuss your work - with your team, your boss, or your entire company - in the same place where you organized and created it.  |  Link: Confluence 101: create content with pages, Description: Think of pages as a New Age ""document."" If Word docs were rotary phones, Confluence pages would be smart phones. A smart phone still makes calls (like their rotary counterparts), but it can do so much more than that  |   0: 59 incomplete Customize the name, colour, and icon of Confluence. 56 incomplete Decide who can see and edit this space or a specific page by clicking the icon. Learn more about Page Restrictions and Space Permissions. 57 incomplete Try adding an inline comment by highlighting some text and click the comment icon. 58 incomplete Learn more about inviting your team to Confluence.  |  ","Welcome to your first space. Go ahead, edit and customize this home page any way you like. We've added some sample content to get you started.###SEP###Goal###SEP###_Your space homepage should summarize what the space is for, and provide links to key resources for your team._###SEP###Core team###SEP###Roadmap You can edit this roadmap or create a new one by adding the Roadmap Planner macro from the Insert menu. Link your Confluence pages to each bar to add visibility, and find more tips by reading the Atlassian blog: [Plan better in 2015 with the Roadmap Planner macro](http://blogs.atlassian.com/2015/01/roadmap-planner-macro/). true%7B%22title%22%3A%22Roadmap%20Planner%22%2C%22timeline%22%3A%7B%22startDate%22%3A%222015-06-01%2000%3A00%3A00%22%2C%22endDate%22%3A%222015-12-31%2000%3A00%3A00%22%2C%22displayOption%22%3A%22MONTH%22%7D%2C%22lanes%22%3A%5B%7B%22title%22%3A%22Marketing%22%2C%22color%22%3A%7B%22lane%22%3A%22%23f15c75%22%2C%22bar%22%3A%22%23f58598%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Social%20campaign%22%2C%22description%22%3A%22Add%20a%20description%20to%20your%20bars%20here.%22%2C%22startDate%22%3A%222015-07-30%2011%3A10%3A05%22%2C%22duration%22%3A3.6435643564356437%2C%22rowIndex%22%3A0%2C%22id%22%3A%22e703c6a8-1649-4d20-9ccf-2c7a8698e385%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22title%22%3A%22Website%20development%22%2C%22description%22%3A%22Add%20a%20description%20to%20your%20bars%20here.%22%2C%22startDate%22%3A%222015-07-17%2006%3A24%3A57%22%2C%22duration%22%3A3.3069306930693068%2C%22rowIndex%22%3A1%2C%22id%22%3A%22655d454d-b701-4584-a301-9ea0bb86ed32%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222015-06-01%2000%3A00%3A00%22%2C%22id%22%3A%22c420ef33-ae28-4828-958f-8a9d793153b3%22%2C%22title%22%3A%22Crowdfunding%20campaign%22%2C%22description%22%3A%22Add%20a%20description%20to%20your%20bars%20here.%22%2C%22duration%22%3A2.5544554455445545%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22People%22%2C%22color%22%3A%7B%22lane%22%3A%22%23654982%22%2C%22bar%22%3A%22%238c77a1%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Recruitment%22%2C%22description%22%3A%22%22%2C%22startDate%22%3A%222015-06-01%2000%3A00%3A00%22%2C%22duration%22%3A2.5%2C%22rowIndex%22%3A0%2C%22id%22%3A%221230bab8-718c-47da-903a-2cbdcb220d97%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-08-17%2013%3A46%3A55%22%2C%22id%22%3A%228639d09c-59d1-4d1f-ad91-c78f04b20135%22%2C%22title%22%3A%22Assessment%20Period%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.910891089108911%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222015-09-01%2021%3A23%3A10%22%2C%22id%22%3A%22802b53f7-ba66-4415-984d-efef93b4caec%22%2C%22title%22%3A%22Training%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.5841584158415842%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-11-15%2006%3A10%3A41%22%2C%22id%22%3A%22502fac56-3849-415f-b412-af27c39229b7%22%2C%22title%22%3A%22Finalisation%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.4356435643564356%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Product%22%2C%22color%22%3A%7B%22lane%22%3A%22%233b7fc4%22%2C%22bar%22%3A%22%236c9fd3%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-06-24%2004%3A02%3A22%22%2C%22id%22%3A%2200ada54b-0998-41a5-aa98-712ecdec8c7f%22%2C%22title%22%3A%22Planning%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.1782178217821784%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-08-31%2001%3A54%3A03%22%2C%22id%22%3A%2271967f2c-f3ab-4871-aaf5-7cf31389e62f%22%2C%22title%22%3A%22Development%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.9207920792079207%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-10-29%2013%3A04%3A09%22%2C%22id%22%3A%22d76ac773-3ee7-495b-9d7f-1daf267dc58c%22%2C%22title%22%3A%22Testing%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222015-11-30%2002%3A36%3A49%22%2C%22id%22%3A%224f584dc6-63b8-4efa-a98e-a5d7bbe9910e%22%2C%22title%22%3A%22Deploy%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.0297029702970297%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%5D%2C%22markers%22%3A%5B%7B%22title%22%3A%22Yearly%20Finalisation%22%2C%22markerDate%22%3A%222015-11-29%2012%3A21%3A23%22%7D%5D%7DRoadmap%20Plannerf0477dfac6f6ca380d8c5f2f44041947###SEP###Know your spaces Everything your team is working on - meeting notes and agendas, project plans and timelines, technical documentation and more - is located in a space; it's home base for your team. A small team should plan to have a space for the team, and a space for each big project. If you'll be working in Confluence with several other teams and departments, we recommend a space for each team as well as a space for each major cross-team project. The key is to think of a space as the container that holds all the important stuff - like pages, files, and blog posts - a team, group, or project needs to work.###SEP###Know your pages If you're working on something related to your team - project plans, product requirements, blog posts, internal communications, you name it - create and store it in a Confluence page. Confluence pages offer a lot of flexibility in creating and storing information, and there are a number of useful page templates included to get you started, like the meeting notes template. Your spaces should be filled with pages that document your business processes, outline your plans, contain your files, and report on your progress. The more you learn to do in Confluence (adding tables and graphs, or embedding video and links are great places to start), the more engaging and helpful your pages will become. Learn more by reading [Confluence 101: organize your work in spaces](https://www.atlassian.com/collaboration/confluence-organize-work-in- spaces)###SEP###Quick navigation When you create new pages in this space, they'll appear here automatically.###SEP###Useful links Tasks###SEP###0: Harvey Honner-whiteTeam Lead, 1: Alana Baczewski Tech Lead, 2: Sameer Farrell Marketing, 3: Mia Bednarczyk Recruitment  |  ###SEP###Link: Confluence 101: organize your work in spaces, Description: Chances are, the information you need to do your job lives in multiple places. Word docs, Evernote files, email, PDFs, even Post-it notes. It's scattered among different systems. And to make matters worse, the stuff your teammates need is equally siloed. If information had feelings, it would be lonely.But with Confluence, you can bring all that information into one place.  |  Link: Confluence 101: discuss work with your team, Description: Getting a project outlined and adding the right content are just the first steps. Now it's time for your team to weigh in. Confluence makes it easy to discuss your work - with your team, your boss, or your entire company - in the same place where you organized and created it.  |  Link: Confluence 101: create content with pages, Description: Think of pages as a New Age ""document."" If Word docs were rotary phones, Confluence pages would be smart phones. A smart phone still makes calls (like their rotary counterparts), but it can do so much more than that  |  ###SEP###0: 59 incomplete Customize the name, colour, and icon of Confluence. 56 incomplete Decide who can see and edit this space or a specific page by clicking the icon. Learn more about Page Restrictions and Space Permissions. 57 incomplete Try adding an inline comment by highlighting some text and click the comment icon. 58 incomplete Learn more about inviting your team to Confluence.  |  ###SEP###"
1310798,"To install torch-xla To setup device to TPU To install required libraries To download en_core_web_md pipeline To clone the repository To test the model when context and questions are given by user To test the model when using Confluence and Jira API  |  ###code### To install torch-xla : pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl ###code### To setup device to TPU : import torch_xla, import torch_xla.core.xla_model as xm, device = xm.xla_device() ###code### To install required libraries : pip install html2text, pip install bert-extractive-summarizer, pip install spacy, pip install transformers, pip install neuralcoref, pip install sentencepiece ###code### To download en_core_web_md pipeline : python -m spacy download en_core_web_md ###code### To clone the repository : git clone --branch bert_model https://<token>@github.com/dss-0620/DB_Chat-Bot_Project.git ###code### To test the model when context and questions are given by user : python <path_to_project_dir>/predict.py ###code### To test the model when using Confluence and Jira API : python <path_to_project_dir>/main.py  |  ","To install torch-xla###SEP###To setup device to TPU###SEP###To install required libraries###SEP###To download en_core_web_md pipeline###SEP###To clone the repository###SEP###To test the model when context and questions are given by user###SEP###To test the model when using Confluence and Jira API###SEP######SEP######code### To install torch-xla : pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl###SEP######code### To setup device to TPU : import torch_xla, import torch_xla.core.xla_model as xm, device = xm.xla_device()###SEP######code### To install required libraries : pip install html2text, pip install bert-extractive-summarizer, pip install spacy, pip install transformers, pip install neuralcoref, pip install sentencepiece###SEP######code### To download en_core_web_md pipeline : python -m spacy download en_core_web_md###SEP######code### To clone the repository : git clone --branch bert_model https://<token>@github.com/dss-0620/DB_Chat-Bot_Project.git###SEP######code### To test the model when context and questions are given by user : python <path_to_project_dir>/predict.py###SEP######code### To test the model when using Confluence and Jira API : python <path_to_project_dir>/main.py"
1310791,"  |    |  Company: Alfreds Futterkiste, Contact: Maria Anders, Country: Germany  |  Company: Centro comercial Moctezuma, Contact: Francisco Chang, Country: Mexico  |  Company: Ernst Handel, Contact: Roland Mendel, Country: Austria  |  Company: Island Trading, Contact: Helen Bennett, Country: UK  |  Company: Laughing Bacchus Winecellars, Contact: Yoshi Tannamuri, Country: Canada  |  Company: Magazzini Alimentari Riuniti, Contact: Giovanni Rovelli, Country: Italy  |  ","###SEP###Company: Alfreds Futterkiste, Contact: Maria Anders, Country: Germany  |  Company: Centro comercial Moctezuma, Contact: Francisco Chang, Country: Mexico  |  Company: Ernst Handel, Contact: Roland Mendel, Country: Austria  |  Company: Island Trading, Contact: Helen Bennett, Country: UK  |  Company: Laughing Bacchus Winecellars, Contact: Yoshi Tannamuri, Country: Canada  |  Company: Magazzini Alimentari Riuniti, Contact: Giovanni Rovelli, Country: Italy  |  ###SEP###"
1278045,loss = loss + weight decay parameter L2 norm of the weights.  |    |  ,"TRAIN_PATH = Path to SQUAD v2 training dataset VAL_PATH = Path to SQUAD v2 validation dataset MODEL_TYPE = Pretrained Model to be used from Huggingface Transformers OUT_DIR = Path to store checkpoints while training the model NUM_EPOCHS = Number of epochs to train the model PER_DEVICE_TRAIN_BATCH_SIZE = Training batch size per device PER_DEVICE_VAL_BATCH_SIZE = Validation batch size per device WARMUP_STEPS = Learning rate will be increase slowly during first {WARMUP_STEPS} number of steps WEIGHT_DECAY = Weight decay is a regularization technique by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function. loss = loss + weight decay parameter L2 norm of the weights. LOG_DIR = Path to store Logs LOGGING_STEPS = Number of steps between two logs MODEL_PATH = Path to store the model DO_TRAIN = Flag to indicate whether to train the model or not DO_EVAL = Flag to indicate whether to evaluate the model or not PRINT_ALL_ANSWERS = Flag to indicate whether to print all the answers or only the one with the maximum probability USERNAME = Email ID for Confluence & Jira TOKEN = Token for Confluence and Jira DOC_RETRIEVER_KEY = From where you need to retrieve docs (Confluence or Jira) CONFLUENCE = Dictionary for Confluence specific configuration JIRA = Dictionary for Jira specific configuration###SEP######SEP###"
